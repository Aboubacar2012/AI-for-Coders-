{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aboubacar2012/AI-for-Coders-/blob/main/Aboubacar_Coding_Challenge_for_Fatima_Fellowship_2023.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBpjBBZc6IvA"
      },
      "source": [
        "# Fatima Fellowship Coding Challenge (Pick 1)\n",
        "\n",
        "Thank you for applying to the Fatima Fellowship. To help us select the Fellows and assess your ability to do machine learning research, we are asking that you complete a short coding challenge. Please pick **1 of these 5** coding challenges, whichever is most aligned with your interests. These coding challenges are not meant to take too long, do NOT spend more than 4-6 hours on them -- you can submit whatever you have.\n",
        "\n",
        "**How to submit**: Please make a copy of this colab notebook, add your code and results, and submit your colab notebook along with your application. If you have never used a colab notebook, [check out this video](https://www.youtube.com/watch?v=i-HnvsehuSw)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### **Important**: Beore you get started, please make sure to make a **copy of this notebook** and set sharing permissions so that **anyone with the link can view**. Otherwise, we will NOT be able to assess your application.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "lQNUZjvuRt-m"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "braBzmRpMe7_"
      },
      "source": [
        "# 1. Deep Learning for Vision"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1IWw-NZf5WfF"
      },
      "source": [
        "**Generated by AI detector**: Train a model to detect if images are generated by AI\n",
        "\n",
        "* Find a dataset of natural images and images generated by AI (here is one such dataset on the [Hugging Face Hub](https://huggingface.co/datasets/competitions/aiornot) but you're welcome to use any dataset you've found.\n",
        "* Create a training and test set.\n",
        "* Build a neural network (using Tensorflow, PyTorch, or any framework you like)\n",
        "* Train it to classify the image as being generated by an AI or not until a reasonable accuracy is reached\n",
        "* [Upload the the model to the Hugging Face Hub](https://huggingface.co/docs/hub/adding-a-model), and add a link to your model below.\n",
        "* Look at some of the images that were classified incorrectly. Please explain what you might do to improve your model's performance on these images in the future (you do not need to impelement these suggestions)\n",
        "\n",
        "**Submission instructions**: Please write your code below and include some examples of images that were classified"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### WRITE YOUR CODE TO TRAIN THE MODEL HERE"
      ],
      "metadata": {
        "id": "K2GJaYBpw91T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Write up**: \n",
        "* Link to the model on Hugging Face Hub: \n",
        "* Include some examples of misclassified images. Please explain what you might do to improve your model's performance on these images in the future (you do not need to impelement these suggestions)\n",
        "\n",
        "[Please put your write up here]"
      ],
      "metadata": {
        "id": "qSeLed2JxvGI"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFU9LTOyMiMj"
      },
      "source": [
        "# 2. Deep Learning for NLP\n",
        "\n",
        "**Fake news classifier**: Train a text classification model to detect fake news articles!\n",
        "\n",
        "* Download the dataset here: https://www.kaggle.com/datasets/sadikaljarif/fake-news-detection-dataset-english (if you'd like, you can also look at fake news datasets in other languages, which are available on the Huggingface Hub)\n",
        "* Develop an NLP model for classification that uses a pretrained language model and the *text* of the article. It should *NOT* use the URL\n",
        "* Finetune your model on the dataset, and generate an AUC curve of your model on the test set of your choice. \n",
        "* [Upload the the model to the Hugging Face Hub](https://huggingface.co/docs/hub/adding-a-model), and add a link to your model below.\n",
        "* *Answer the following question*: Look at some of the news articles that were classified incorrectly. Please explain what you might do to improve your model's performance on these news articles in the future (you do not need to impelement these suggestions)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lFwNzy4xStm2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YwBYN1RfS2Nk",
        "outputId": "55aef2eb-96ab-4d6d-c607-0b2e90b316b7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qzfh07tAWXvl",
        "outputId": "4b72aaea-b790-4cbc-f078-cc5f89e4b547"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.27.1-py3-none-any.whl (6.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.13.2-py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.2/199.2 KB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m40.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.10.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.13.2 tokenizers-0.13.2 transformers-4.27.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "\n",
        "# Load data\n",
        "true_df = pd.read_csv('/content/drive/MyDrive/ml_data/nlp/True.csv')\n",
        "fake_df = pd.read_csv('/content/drive/MyDrive/ml_data/nlp/Fake.csv')\n",
        "\n",
        "# Add labels\n",
        "true_df['label'] = 1\n",
        "fake_df['label'] = 0\n",
        "\n",
        "# Combine dataframes\n",
        "df = pd.concat([true_df, fake_df], ignore_index=True)\n",
        "\n",
        "# Split into train and test sets\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "# Load tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
        "\n",
        "# Tokenize input data\n",
        "def tokenize_data(data):\n",
        "    return tokenizer(data['text'].tolist(), padding=True, truncation=True)\n",
        "\n",
        "train_encodings = tokenize_data(train_df)\n",
        "test_encodings = tokenize_data(test_df)\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    evaluation_strategy='epoch',\n",
        "    save_strategy='epoch',\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=64,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    push_to_hub=False,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model='roc_auc',\n",
        "    greater_is_better=True\n",
        ")\n",
        "\n",
        "# Define trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_encodings,\n",
        "    eval_dataset=test_encodings,\n",
        "    data_collator=lambda data: {'input_ids': torch.stack([item[0] for item in data]),\n",
        "                                'attention_mask': torch.stack([item[1] for item in data]),\n",
        "                                'labels': torch.tensor([item[2] for item in data])}\n",
        ")\n",
        "\n",
        "# Fine-tune model\n",
        "trainer.train()\n",
        "\n",
        "# Generate predictions\n",
        "predictions, _, _ = trainer.predict(test_encodings)\n",
        "y_pred = np.argmax(predictions, axis=1)\n",
        "y_true = test_df['label'].tolist()\n",
        "\n",
        "# Print classification report and AUC score\n",
        "print(classification_report(y_true, y_pred))\n",
        "print('AUC score:', roc_auc_score(y_true, predictions[:, 1]))\n",
        "\n",
        "# Save model to Hugging Face Hub\n",
        "trainer.push_to_hub('fake-news-classifier-true-fake')\n"
      ],
      "metadata": {
        "id": "E90i018KyJH3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d5946e2-b538-43c2-909c-ab270adefe1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Write up**: \n",
        "* Link to the model on Hugging Face Hub: \n",
        "* Include some examples of misclassified news articles. Please explain what you might do to improve your model's performance on these news articles in the future (you do not need to impelement these suggestions)\n",
        "\n",
        "[Please put your write up here]"
      ],
      "metadata": {
        "id": "kpInVUMLyJ24"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTfHpo6BOmE8"
      },
      "source": [
        "# 3. Deep RL / Robotics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "saB64bbTXWgZ"
      },
      "source": [
        "**RL for Classical Control:** Using any of the [classical control](https://github.com/openai/gym/blob/master/docs/environments.md#classic-control) environments from OpenAI's `gym`, implement a deep NN that learns an optimal policy which maximizes the reward of the environment.\n",
        "\n",
        "* Describe the NN you implemented and the behavior you observe from the agent as the model converges (or diverges).\n",
        "* Plot the reward as a function of steps (or Epochs).\n",
        "Compare your results to a random agent.\n",
        "* Discuss whether you think your model has learned the optimal policy and potential methods for improving it and/or where it might fail.\n",
        "* (Optional) [Upload the the model to the Hugging Face Hub](https://huggingface.co/docs/hub/adding-a-model), and add a link to your model below.\n",
        "\n",
        "\n",
        "You may use any frameworks you like, but you must implement your NN on your own (no pre-defined/trained models like [`stable_baselines`](https://stable-baselines.readthedocs.io/en/master/)).\n",
        "\n",
        "You may use any simulator other than `gym` _however_:\n",
        "* The environment has to be similar to the classical control environments (or more complex like [`robosuite`](https://github.com/ARISE-Initiative/robosuite)).\n",
        "* You cannot choose a game/Atari/text based environment. The purpose of this challenge is to demonstrate an understanding of basic kinematic/dynamic systems."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### WRITE YOUR CODE TO TRAIN THE MODEL HERE"
      ],
      "metadata": {
        "id": "CUhkTcoeynVv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Write up**: \n",
        "* (Optional) link to the model on Hugging Face Hub: \n",
        "* Discuss whether you think your model has learned the optimal policy and potential methods for improving it and/or where it might fail.\n",
        "\n",
        "[Please put your write up here]"
      ],
      "metadata": {
        "id": "bWllPZhJyotg"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbrRbrISa5J_"
      },
      "source": [
        "# 4. Theory / Linear Algebra "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFkLRCzTXTzL"
      },
      "source": [
        "**Implement Contrastive PCA** Read [this paper](https://www.nature.com/articles/s41467-018-04608-8) and implement contrastive PCA in Python.\n",
        "\n",
        "* First, please discuss what kind of dataset this would make sense to use this method on\n",
        "* Implement the method in Python (do not use previous implementations of the method if they already exist)\n",
        "* Then create a synthetic dataset and apply the method to the synthetic data. Compare with standard PCA.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Write up**: Discuss what kind of dataset it would make sense to use Contrastive PCA\n",
        "\n",
        "[Please put your write up here]"
      ],
      "metadata": {
        "id": "TpyqWl-ly0wy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### WRITE YOUR CODE HERE"
      ],
      "metadata": {
        "id": "1CQzUSfQywRk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Systems"
      ],
      "metadata": {
        "id": "dlqmZS5Hy6q-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inference on the edge**: Measure the inference times in various computationally-constrained settings\n",
        "\n",
        "* Pick a few different speech detection models (we suggest looking at models  on the [Hugging Face Hub](https://huggingface.co/models?pipeline_tag=automatic-speech-recognition&sort=downloads))\n",
        "* Simulate different memory constraints and CPU allocations that are realistic for edge devices that might run such models, such as smart speakers or microcontrollers, and measure what is the average inference time of the models under these conditions \n",
        "* How does the inference time vary with (1) choice of model (2) available system memory (3) available CPU (4) size of input?\n",
        "\n",
        "Are there any surprising discoveries? (Note that this coding challenge is fairly open-ended, so we will be considering the amount of effort invested in discovering something interesting here)."
      ],
      "metadata": {
        "id": "QW_eiDFw1QKm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### WRITE YOUR CODE HERE"
      ],
      "metadata": {
        "id": "OYp94wLP1kWJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Write up**: What surprising discoveries do you see?\n",
        "\n",
        "[Please put your write up here]"
      ],
      "metadata": {
        "id": "yoHmutWx2jer"
      }
    }
  ]
}